{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPGqFph0hWNA"
      },
      "source": [
        "## Data loading and cleaning\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CiDn2Sk-VWqE"
      },
      "outputs": [],
      "source": [
        "# # Run this to use from colab environment\n",
        "# !pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NE_fTbKGe5z",
        "outputId": "e1614342-6bac-4c4b-d431-f01272b75dfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of samples: 113\n",
            "The number of columns: 160\n"
          ]
        }
      ],
      "source": [
        "from load_data import load_data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, chi2, SequentialFeatureSelector\n",
        "import seaborn\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from statsmodels.stats import weightstats\n",
        "import numpy as np\n",
        "import statistics\n",
        "\n",
        "# Classifiers and kernels\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "from sklearn.metrics.pairwise import rbf_kernel, sigmoid_kernel\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n",
        "\n",
        "features = data.drop(columns=['label'])\n",
        "label = data.label\n",
        "\n",
        "# Splitting data in train and test group\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=.2)\n",
        "\n",
        "# functie van maken??\n",
        "y_train_bin = []\n",
        "for val in y_train:\n",
        "  if val == 'T12':\n",
        "    y_train_bin.append(0)\n",
        "  else:\n",
        "    y_train_bin.append(1) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAa7WJCd56fk"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS8qaMN_7Unb"
      },
      "source": [
        "### Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q6BJaMI37Vq7"
      },
      "outputs": [],
      "source": [
        "# Scale the dataset\n",
        "scaler = RobustScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns = features.columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns = features.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### removing outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the dataframe\n",
        "outlier_feat = []\n",
        "for feature in X_train_scaled.columns:\n",
        "    # IQR\n",
        "    Q1 = np.percentile(X_train_scaled[feature], 25,\n",
        "                    interpolation = 'midpoint')\n",
        "    \n",
        "    Q3 = np.percentile(X_train_scaled[feature], 75,\n",
        "                    interpolation = 'midpoint')\n",
        "    IQR = Q3 - Q1\n",
        " \n",
        "    # Upper bound\n",
        "    X_train_scaled.loc[X_train_scaled[feature] >= (Q3+1.5*IQR),feature] = statistics.median(X_train_scaled[feature])\n",
        "\n",
        "    # Lower bound\n",
        "    X_train_scaled.loc[X_train_scaled[feature] <= (Q3-1.5*IQR),feature] = statistics.median(X_train_scaled[feature])\n",
        "\n",
        "\n",
        "for feature in X_test_scaled.columns:\n",
        "    # IQR\n",
        "    Q1 = np.percentile(X_test_scaled[feature], 25,\n",
        "                    interpolation = 'midpoint')\n",
        "    \n",
        "    Q3 = np.percentile(X_test_scaled[feature], 75,\n",
        "                    interpolation = 'midpoint')\n",
        "    IQR = Q3 - Q1\n",
        " \n",
        "    # Upper bound\n",
        "    X_test_scaled.loc[X_test_scaled[feature] >= (Q3+1.5*IQR),feature] = statistics.median(X_test_scaled[feature])\n",
        "    # Lower bound\n",
        "    X_test_scaled.loc[X_test_scaled[feature] <= (Q3-1.5*IQR),feature] = statistics.median(X_test_scaled[feature])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V55PGYOyPp0A"
      },
      "source": [
        "## Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQsEWuviPvxG"
      },
      "source": [
        "### T-test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "PXDFa7mVPyd2",
        "outputId": "aca9c33a-f77f-4dab-b235-413a806be361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of significant different features: 18\n"
          ]
        }
      ],
      "source": [
        "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns = X_train.columns) # make df from numpy\n",
        "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns = X_train.columns)\n",
        "X_train_scaled_df['Label'] = y_train_bin\n",
        "X_train_T12 = X_train_scaled_df.groupby('Label').get_group(0)\n",
        "X_train_T34 = X_train_scaled_df.groupby('Label').get_group(1)\n",
        "X_train_T12 = X_train_T12.drop(columns = ['Label'])\n",
        "X_train_T34 = X_train_T34.drop(columns = ['Label'])\n",
        "\n",
        "# ttest\n",
        "_,pval = stats.ttest_ind(X_train_T12,X_train_T34)\n",
        "\n",
        "\n",
        "\n",
        "sig_feat = []\n",
        "for id, val in enumerate(pval):\n",
        "  if val < 0.05/X_train_scaled_df.shape[1]:\n",
        "    sig_feat.append(list(X_train.columns)[id])\n",
        "print(f'Number of significant different features: {len(sig_feat)}')\n",
        "\n",
        "X_train_sig = X_train_scaled_df[sig_feat]\n",
        "X_test_sig = X_test_scaled_df[sig_feat]\n",
        "\n",
        "# # Pairplot of sign features\n",
        "# X_train_sig.columns =['Feature'+ str(pc) for pc in range(1,len(sig_feat)+1)]\n",
        "# X_train_sig['Grade'] = y_train_bin\n",
        "# pair_plot = seaborn.pairplot(X_train_sig, hue = 'Grade')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cfs = RandomForestClassifier(n_estimators=5, bootstrap=True)\n",
        "\n",
        "\n",
        "# t_it = np.zeros(159)\n",
        "# for it in range(20):\n",
        "#     print(it)\n",
        "#     cfs = KNeighborsClassifier(n_neighbors=5)\n",
        "#     sfs = SequentialFeatureSelector(cfs,n_features_to_select=5)\n",
        "#     X_train_fs = sfs.fit_transform(X_train_scaled, y_train)\n",
        "#     it = sfs.get_support()\n",
        "#     t_it = np.vstack([t_it,it])\n",
        "\n",
        "\n",
        "# voting = np.sum(t_it, axis=0)\n",
        "\n",
        "# THRES = 10\n",
        "# sig_feat = []\n",
        "# for id, value in enumerate(voting):\n",
        "#     if value > THRES:\n",
        "#         sig_feat.append(list(X_train.columns)[id])\n",
        "\n",
        "# X_train_scaled_df = pd.DataFrame(X_train_scaled, columns = X_train.columns) # make df from numpy\n",
        "# X_test_scaled_df = pd.DataFrame(X_test_scaled, columns = X_train.columns)\n",
        "# X_train_sig = X_train_scaled_df[sig_feat]\n",
        "# X_test_sig = X_test_scaled_df[sig_feat]\n",
        "\n",
        "# X_train_sig\n",
        "# # X_test_fs = sfs.transform(X_test_scaled)\n",
        "\n",
        "cfs = KNeighborsClassifier(n_neighbors=5)\n",
        "sfs = SequentialFeatureSelector(cfs,n_features_to_select=5)\n",
        "X_train_fs = sfs.fit_transform(X_train_scaled, y_train)\n",
        "X_test_fs = sfs.transform(X_test_scaled)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4emhz-AV_yGv"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qR_LfjKs-V9s"
      },
      "outputs": [],
      "source": [
        "N_COMP = 10\n",
        "pca = PCA(n_components=N_COMP)\n",
        "pca.fit(X_train_sig)\n",
        "X_train_pca = pca.transform(X_train_sig)\n",
        "X_test_pca = pca.transform(X_test_sig)\n",
        "\n",
        "\n",
        "# seaborn.scatterplot(x=X_train_pca[:,0],y=X_train_pca[:,1],hue=y_train)\n",
        "# scatter_data = pd.DataFrame(X_train_pca[:,:], columns = ['Principal component' + str(pc) for pc in range(1,N_COMP+1)])\n",
        "# scatter_data['Stage'] = y_train_bin\n",
        "# seaborn.pairplot(scatter_data, hue = 'Stage')\n",
        "# print(scatter_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQWNGavlBlF0"
      },
      "source": [
        "# Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87ztLj3EAJyU",
        "outputId": "ca47719e-ce58-4b90-d5dd-61fb8abcf6d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training result kNN: 0.7333333333333333\n",
            "Test result kNN: 0.7391304347826086\n",
            "Training result Random Forest: 0.9777777777777777\n",
            "Test result Random Forest: 0.7391304347826086\n"
          ]
        }
      ],
      "source": [
        "# Construct classifiers\n",
        "svmlin = SVC(kernel='linear', gamma='scale')\n",
        "svmrbf = SVC(kernel='rbf', gamma='scale')\n",
        "svmpoly = SVC(kernel='poly', degree=3, gamma='scale')\n",
        "\n",
        "clsfs = [KNeighborsClassifier(), RandomForestClassifier(),QuadraticDiscriminantAnalysis(),GaussianNB(),LinearDiscriminantAnalysis(),svmlin, svmpoly, svmrbf]\n",
        "\n",
        "for clf in clsfs:\n",
        "    # Fit classifier\n",
        "    clf.fit(X_train_sig,y_train)\n",
        "    y_pred_train=clf.predict(X_train_sig)\n",
        "    print(clf)\n",
        "    acc_train = (y_train==y_pred_train).sum()/len(X_train_sig)\n",
        "    print(f'Train data acc: {acc_train}')\n",
        "    y_pred_test = clf.predict(X_test_sig)\n",
        "    acc_test = (y_test==y_pred_test).sum()/len(X_test_sig)\n",
        "    print(f'Test data acc: {acc_test}')\n",
        "## Optimization\n",
        "### KNN\n",
        "# Create a 20 fold stratified CV iterator\n",
        "cv_20fold = model_selection.StratifiedKFold(n_splits=5)\n",
        "results = []\n",
        "best_n_neighbors = []\n",
        "X_train_sig_a = X_train_sig.to_numpy()\n",
        "y_train_a = y_train.to_numpy()\n",
        "\n",
        "# Loop over the folds\n",
        "for validation_index, test_index in cv_20fold.split(X_train_sig_a,y_train_a):\n",
        "    # Split the data properly\n",
        "    X_validation = X_train_sig_a[validation_index]\n",
        "    y_validation = y_train_a[validation_index]\n",
        "    \n",
        "    X_test_op = X_train_sig_a[test_index]\n",
        "    y_test_op = y_train_a[test_index]\n",
        "    \n",
        "    # Create a grid search to find the optimal k using a gridsearch and 10-fold cross validation\n",
        "    # Same as above\n",
        "    parameters = {\"n_neighbors\": list(range(1, 31, 2))}\n",
        "    knn = KNeighborsClassifier()\n",
        "    cv_10fold = model_selection.StratifiedKFold(n_splits=5)\n",
        "    grid_search = model_selection.GridSearchCV(knn, parameters, cv=cv_10fold, scoring='roc_auc')\n",
        "    grid_search.fit(X_validation, y_validation)\n",
        "    \n",
        "    # Get resulting classifier\n",
        "    clf = grid_search.best_estimator_\n",
        "    print(f'Best classifier: k={clf.n_neighbors}')\n",
        "    best_n_neighbors.append(clf.n_neighbors)\n",
        "    \n",
        "    # Test the classifier on the test data\n",
        "    probabilities = clf.predict_proba(X_test_op)\n",
        "    scores = probabilities[:, 1]\n",
        "    \n",
        "    # Get the auc\n",
        "    auc = metrics.roc_auc_score(y_test_op, scores)\n",
        "    results.append({\n",
        "        'auc': auc,\n",
        "        'k': clf.n_neighbors,\n",
        "        'set': 'test'\n",
        "    })\n",
        "    \n",
        "    # Test the classifier on the validation data\n",
        "    probabilities_validation = clf.predict_proba(X_validation)\n",
        "    scores_validation = probabilities_validation[:, 1]\n",
        "    \n",
        "    # Get the auc\n",
        "    auc_validation = metrics.roc_auc_score(y_validation, scores_validation)\n",
        "    results.append({\n",
        "        'auc': auc_validation,\n",
        "        'k': clf.n_neighbors,\n",
        "        'set': 'validation'\n",
        "    })\n",
        "    \n",
        "# Create results dataframe and plot it\n",
        "results = pd.DataFrame(results)\n",
        "# seaborn.boxplot(y='auc', x='set', data=results)\n",
        "\n",
        "optimal_n = int(np.median(best_n_neighbors))\n",
        "print(f\"The optimal N={optimal_n}\")\n",
        "# print(results)\n",
        "\n",
        "\n",
        "# Use the optimal parameters without any tuning to validate the optimal classifier\n",
        "clf = KNeighborsClassifier(n_neighbors=optimal_n)\n",
        "\n",
        "# Fit on the entire dataset\n",
        "clf.fit(X_train_sig, y_train)\n",
        "\n",
        "# Test the classifier on the indepedent replication data\n",
        "probabilities = clf.predict_proba(X_test_sig)\n",
        "scores = probabilities[:, 1]\n",
        "\n",
        "# Get the auc\n",
        "auc = metrics.roc_auc_score(y_test, scores)\n",
        "print(f'THe AUC on the replication set is {auc} using a {clf.n_neighbors}-NN classifier')\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=optimal_n)\n",
        "knn.fit(X_train_sig, y_train)\n",
        "score_train_kNN = knn.score(X_train_sig, y_train)\n",
        "score_test_kNN = knn.score(X_test_sig, y_test)\n",
        "print(f\"Training result kNN: {score_train_kNN}\")\n",
        "print(f\"Test result kNN: {score_test_kNN}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnfuoX7LB8D7"
      },
      "outputs": [],
      "source": [
        "# Create a 20 fold stratified CV iterator\n",
        "cv_20fold = model_selection.StratifiedKFold(n_splits=5)\n",
        "results = []\n",
        "best_n_neighbors = []\n",
        "X_train_sig_a = X_train_sig.to_numpy()\n",
        "y_train_a = y_train.to_numpy()\n",
        "\n",
        "# Loop over the folds\n",
        "for validation_index, test_index in cv_20fold.split(X_train_sig_a,y_train_a):\n",
        "    # Split the data properly\n",
        "    X_validation = X_train_sig_a[validation_index]\n",
        "    y_validation = y_train_a[validation_index]\n",
        "    \n",
        "    X_test = X_train_sig_a[test_index]\n",
        "    y_test = y_train_a[test_index]\n",
        "    \n",
        "    # Create a grid search to find the optimal k using a gridsearch and 10-fold cross validation\n",
        "    # Same as above\n",
        "    parameters = {\"n_estimators\": list(range(1, 50))}\n",
        "    rf = RandomForestClassifier(criterion= \"gini\", bootstrap = True, min_samples_leaf = 5)\n",
        "    cv_10fold = model_selection.StratifiedKFold(n_splits=5)\n",
        "    grid_search = model_selection.GridSearchCV(rf, parameters, cv=cv_10fold, scoring='roc_auc')\n",
        "    grid_search.fit(X_validation, y_validation)\n",
        "    \n",
        "    # Get resulting classifier\n",
        "    clf = grid_search.best_estimator_\n",
        "    print(f'Best classifier: n={clf.n_estimators}')\n",
        "    best_n_neighbors.append(clf.n_estimators)\n",
        "    \n",
        "    # Test the classifier on the test data\n",
        "    probabilities = clf.predict_proba(X_test)\n",
        "    scores = probabilities[:, 1]\n",
        "    \n",
        "    # Get the auc\n",
        "    auc = metrics.roc_auc_score(y_test, scores)\n",
        "    results.append({\n",
        "        'auc': auc,\n",
        "        'n': clf.n_estimators,\n",
        "        'set': 'test'\n",
        "    })\n",
        "    \n",
        "    # Test the classifier on the validation data\n",
        "    probabilities_validation = clf.predict_proba(X_validation)\n",
        "    scores_validation = probabilities_validation[:, 1]\n",
        "    \n",
        "    # Get the auc\n",
        "    auc_validation = metrics.roc_auc_score(y_validation, scores_validation)\n",
        "    results.append({\n",
        "        'auc': auc_validation,\n",
        "        'n': clf.n_estimators,\n",
        "        'set': 'validation'\n",
        "    })\n",
        "    \n",
        "# Create results dataframe and plot it\n",
        "results = pd.DataFrame(results)\n",
        "seaborn.boxplot(y='auc', x='set', data=results)\n",
        "\n",
        "optimal_n = int(np.median(best_n_neighbors))\n",
        "print(f\"The optimal N={optimal_n}\")\n",
        "print(results)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "H&N group 12.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
