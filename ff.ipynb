{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binner(y, class1):\n",
    "    '''\n",
    "    Transform y_label to binary\n",
    "    '''\n",
    "    y_bin = []\n",
    "    for val in y:\n",
    "        if val == class1:\n",
    "            y_bin.append(0)\n",
    "        else:\n",
    "            y_bin.append(1) \n",
    "    return y_bin\n",
    "def plot_roc(y_score, y_truth, output_dir, title):\n",
    "    '''\n",
    "    Plot an ROC curve.\n",
    "    '''\n",
    "    # Only take scores for class = 1\n",
    "    y_score = y_score[:, 1]\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_truth, y_score)\n",
    "    roc_auc = metrics.auc(fpr, tpr)     \n",
    "    \n",
    "    # Plot the ROC curve\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(output_dir)\n",
    "def eval_func(features, label):\n",
    "    '''\n",
    "    Entire script for evaluation set up. \n",
    "    Note: Same code as in next sections!\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=.2, stratify=label)\n",
    "    \n",
    "    y_train_bin = binner(y_train, 'T12') # make binary y_score\n",
    "    y_test_bin = binner(y_test, 'T12')\n",
    "\n",
    "# Outlier detection\n",
    "    for feature in X_train.columns:\n",
    "        Q1 = np.percentile(X_train[feature], 25, interpolation = 'midpoint') # Defining quartiles\n",
    "        Q3 = np.percentile(X_train[feature], 75, interpolation = 'midpoint')\n",
    "        IQR = Q3 - Q1\n",
    "        if not IQR == 0: # If 0, all values in range, so no adjusting necessary\n",
    "            X_train.loc[X_train[feature] > (Q3+1.5*IQR),feature] = Q3 # Upper bound\n",
    "            X_train.loc[X_train[feature] < (Q1-1.5*IQR),feature] = Q1 # Lower bound\n",
    "\n",
    "    for feature in X_test.columns:\n",
    "        Q1 = np.percentile(X_test[feature], 25, interpolation = 'midpoint')\n",
    "        Q3 = np.percentile(X_test[feature], 75, interpolation = 'midpoint')\n",
    "        IQR = Q3 - Q1\n",
    "        if not IQR == 0:\n",
    "            X_test.loc[X_test[feature] > (Q3+1.5*IQR),feature] = Q3 \n",
    "            X_test.loc[X_test[feature] < (Q1-1.5*IQR),feature] = Q1\n",
    "\n",
    "    # Scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns = features.columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns = features.columns)\n",
    "\n",
    "    coefs = []\n",
    "    accuracies = []\n",
    "    times = []\n",
    "    n_alphas = 100\n",
    "    alphas = np.logspace(-7, -1, n_alphas)\n",
    "\n",
    "    for a in alphas:\n",
    "        # Fit classifier\n",
    "        clf = Lasso(alpha=a, fit_intercept=False,tol=0.044)\n",
    "        clf.fit(X_train_scaled, y_train_bin) \n",
    "        y_pred = clf.predict(X_test_scaled)\n",
    "        \n",
    "        # Append statistics\n",
    "        accuracy = clf.score(X_train_scaled,y_train_bin)\n",
    "        accuracies.append(accuracy)\n",
    "        coefs.append(clf.coef_)\n",
    "\n",
    "    selector = SelectFromModel(estimator=Lasso(alpha=10**(-6), tol=0.002248888888888889), threshold='median')\n",
    "    selector.fit(X_train_scaled, y_train_bin)\n",
    "    n_original = X_train_scaled.shape[1]\n",
    "    X_train_fs = selector.transform(X_train_scaled)\n",
    "    X_test_fs = selector.transform(X_test_scaled)\n",
    "    n_selected = X_train_fs.shape[1]\n",
    "\n",
    "    N_COMP = .9 # Capture components at 90% of the variance\n",
    "    pca = PCA(n_components=N_COMP)\n",
    "    pca.fit(X_train_fs)\n",
    "    X_train_pca = pca.transform(X_train_fs)\n",
    "    X_test_pca = pca.transform(X_test_fs)\n",
    "\n",
    "    # SVC\n",
    "    # Create a 6 fold stratified CV iterator\n",
    "    cv_6fold = StratifiedKFold(n_splits=6)\n",
    "    results = []\n",
    "    best_cls = []\n",
    "    y_train_a = y_train.to_numpy()\n",
    "\n",
    "    # Loop over the folds\n",
    "    for train_opt_index, validation_index in cv_6fold.split(X_train_pca,y_train_a):\n",
    "        # Split the data properly\n",
    "        X_train_opt = X_train_pca[train_opt_index]\n",
    "        y_train_opt = y_train_a[train_opt_index]\n",
    "        \n",
    "        X_validation = X_train_pca[validation_index]\n",
    "        y_validation = y_train_a[validation_index]\n",
    "        \n",
    "        # Create a grid search to find the optimal k using a gridsearch and 3-fold cross validation\n",
    "        # Same as above\n",
    "        parameters = {\n",
    "            'C':list(np.linspace(0.01,1,100)),\n",
    "        }\n",
    "        svm_clf = SVC(kernel = 'linear', probability=True)\n",
    "        cv_3fold = StratifiedKFold(n_splits=3)\n",
    "        grid_search = GridSearchCV(svm_clf, parameters, cv=cv_3fold, scoring='roc_auc')\n",
    "        grid_search.fit(X_train_opt, y_train_opt)\n",
    "        \n",
    "        # Get resulting classifier\n",
    "        clf = grid_search.best_estimator_\n",
    "\n",
    "        best_cls.append(clf.C)\n",
    "        \n",
    "        # Test the classifier on the train_opt data\n",
    "        probabilities_train_opt = clf.predict_proba(X_train_opt)\n",
    "        scores_train_opt = probabilities_train_opt[:, 1]\n",
    "        \n",
    "        # Get the auc\n",
    "        auc_train_opt = metrics.roc_auc_score(y_train_opt, scores_train_opt)\n",
    "        results.append({\n",
    "            'AUC': auc_train_opt,\n",
    "            'n': clf.C,\n",
    "            'Set': 'Train'\n",
    "        })\n",
    "\n",
    "        # Test the classifier on the test data\n",
    "        probabilities = clf.predict_proba(X_validation)\n",
    "        scores = probabilities[:, 1]\n",
    "        \n",
    "        # Get the auc\n",
    "        auc = metrics.roc_auc_score(y_validation, scores)\n",
    "        results.append({\n",
    "            'AUC': auc,\n",
    "            'n': clf.C,\n",
    "            'Set': 'Validation'\n",
    "        })\n",
    "        \n",
    "\n",
    "    optimal_c = float(np.mean(best_cls))\n",
    "\n",
    "\n",
    "        # Use the optimal parameters without any tuning to validate the optimal classifier\n",
    "    clf = SVC(kernel = 'linear', probability=True, C=optimal_c)\n",
    "    # Fit on the entire dataset\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "\n",
    "    # Test the classifier on the indepedent test data\n",
    "    probabilities_train = clf.predict_proba(X_train_pca)\n",
    "    probabilities_test = clf.predict_proba(X_test_pca)\n",
    "    scores_train = probabilities_train[:, 1]\n",
    "    scores_test = probabilities_test[:, 1]\n",
    "\n",
    "    # Get the auc, sensitivity and specificity\n",
    "    auc_svc_train = metrics.roc_auc_score(y_train, scores_train)\n",
    "    auc_svc_test = metrics.roc_auc_score(y_test, scores_test)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_test, clf.predict(X_test_pca)).ravel()\n",
    "    specificity_svc = tn / (tn+fp)\n",
    "    sensitivity_svc = tp/(tp+fn)\n",
    "\n",
    "\n",
    "    # KNN\n",
    "    # Create a 6 fold stratified CV iterator\n",
    "    cv_6fold = StratifiedKFold(n_splits=6)\n",
    "    results = []\n",
    "    best_cls = []\n",
    "    y_train_a = y_train.to_numpy()\n",
    "\n",
    "    # Loop over the folds\n",
    "    for train_opt_index, validation_index in cv_6fold.split(X_train_pca,y_train_a):\n",
    "        # Split the data properly\n",
    "        X_train_opt = X_train_pca[train_opt_index]\n",
    "        y_train_opt = y_train_a[train_opt_index]\n",
    "        \n",
    "        X_validation = X_train_pca[validation_index]\n",
    "        y_validation = y_train_a[validation_index]\n",
    "        \n",
    "        # Create a grid search to find the optimal k using a gridsearch and 3-fold cross validation\n",
    "        # Same as above\n",
    "        parameters = {\n",
    "            'n_neighbors': list(range(1,31)),\n",
    "        }\n",
    "        knn_clf = KNeighborsClassifier(weights='distance')\n",
    "        cv_3fold = StratifiedKFold(n_splits=3)\n",
    "        grid_search = GridSearchCV(knn_clf, parameters, cv=cv_3fold, scoring='roc_auc')\n",
    "        grid_search.fit(X_train_opt, y_train_opt)\n",
    "        \n",
    "        # Get resulting classifier\n",
    "        clf = grid_search.best_estimator_\n",
    "\n",
    "        best_cls.append(clf.n_neighbors)\n",
    "        \n",
    "        # Test the classifier on the train_opt data\n",
    "        probabilities_train_opt = clf.predict_proba(X_train_opt)\n",
    "        scores_train_opt = probabilities_train_opt[:, 1]\n",
    "        \n",
    "        # Get the auc\n",
    "        auc_train_opt = metrics.roc_auc_score(y_train_opt, scores_train_opt)\n",
    "        results.append({\n",
    "            'AUC': auc_train_opt,\n",
    "            'k': clf.n_neighbors,\n",
    "            'Set': 'Train'\n",
    "        })\n",
    "\n",
    "        # Test the classifier on the test data\n",
    "        probabilities = clf.predict_proba(X_validation)\n",
    "        scores = probabilities[:, 1]\n",
    "        \n",
    "        # Get the auc\n",
    "        auc = metrics.roc_auc_score(y_validation, scores)\n",
    "        results.append({\n",
    "            'AUC': auc,\n",
    "            'k': clf.n_neighbors,\n",
    "            'Set': 'Validation'\n",
    "        })\n",
    "        \n",
    "    # Create results dataframe and plot it\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "    optimal_n = int(np.mean(best_cls))\n",
    "\n",
    "\n",
    "    # Use the optimal parameters without any tuning to validate the optimal classifier\n",
    "    clf = KNeighborsClassifier(weights='distance', n_neighbors=optimal_n)\n",
    "    # Fit on the entire dataset\n",
    "    clf.fit(X_train_pca, y_train)\n",
    "\n",
    "    # Test the classifier on the indepedent test data\n",
    "    probabilities_train = clf.predict_proba(X_train_pca)\n",
    "    probabilities_test = clf.predict_proba(X_test_pca)\n",
    "    scores_train = probabilities_train[:, 1]\n",
    "    scores_test = probabilities_test[:, 1]\n",
    "\n",
    "    # Get the auc\n",
    "    auc_knn_train = metrics.roc_auc_score(y_train, scores_train)\n",
    "    auc_knn_test = metrics.roc_auc_score(y_test, scores_test)\n",
    "\n",
    "\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_test, clf.predict(X_test_pca)).ravel()\n",
    "    specificity_knn = tn / (tn+fp)\n",
    "    sensitivity_knn = tp/(tp+fn)\n",
    "\n",
    "\n",
    "    return auc_svc_train, auc_svc_test,sensitivity_svc, specificity_svc, auc_knn_train, auc_knn_test, sensitivity_knn, specificity_knn"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
